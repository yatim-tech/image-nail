job: extension
config:
  name: flux_lora_training
  process:
  - type: sd_trainer
    training_folder: /app/checkpoints
    device: cuda:0
    trigger_word: ""
    
    network:
      type: lora
      linear: 128
      linear_alpha: 128
      # Flux specific network settings
    
    save:
      dtype: bf16
      save_every: 250  # Save every N steps
      max_step_saves_to_keep: 4
      save_format: safetensors
    
    datasets:
    - folder_path: /dataset/images
      caption_ext: txt
      caption_dropout_rate: 0.05
      shuffle_tokens: false
      cache_latents_to_disk: true
      resolution: [512, 768, 1024]
    
    train:
      batch_size: 1
      steps: 2000
      gradient_accumulation_steps: 1
      train_unet: true
      train_text_encoder: false
      gradient_checkpointing: true
      noise_scheduler: flowmatch
      optimizer: adamw8bit
      lr: 0.0004
      
      ema_config:
        use_ema: true
        ema_decay: 0.99
      
      dtype: bf16
      
    model:
      name_or_path: black-forest-labs/FLUX.1-dev
      is_flux: true
      quantize: true

meta:
  name: flux_lora
  version: '1.0'

